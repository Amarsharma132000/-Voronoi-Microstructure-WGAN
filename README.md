Project Name: Synthetic Microstructure Generation using WGAN-GPObjectiveDesign and implement a Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP) to synthesize high-fidelity Voronoi material microstructures. This project addresses "mode collapse" and training instability common in standard DCGANs, utilizing advanced optimization techniques like Mixed Precision and Two-Time-Scale Update Rules (TTUR).1. Architecture & Model DesignApproach:Generator: Custom upsampling network (Latent Dim $\to$ 64$\times$64) utilizing UpSampling2D and Conv2D. notably excluding Batch Normalization to prevent artifacting in texture generation .Critic (Discriminator): Deep convolutional network with strided convolutions and LeakyReLU activations to approximate the Wasserstein distance .Loss Function: Wasserstein Loss with Gradient Penalty ($\lambda=10$) to enforce the 1-Lipschitz constraint, ensuring stable gradients throughout training .Key Achievements:Eliminated "mode collapse," generating diverse grain structures rather than repetitive patterns.Achieved stable convergence where the Critic loss serves as a meaningful metric of generation quality.Generated sharp, realistic grain boundaries and triple junctions.2. Data Pipeline & PreprocessingApproach:Data Ingestion: Automated H5 file parsing and stacking from compressed datasets .Preprocessing: Data reshaping and normalization to $[-1, 1]$ range to match the Generator's tanh output.Pipeline Optimization: Utilized tf.data.Dataset with shuffling and batching for efficient GPU streaming.Key Achievements:Seamless handling of high-dimensional microstructure datasets.Optimized memory usage allowing for larger batch sizes (64) on limited hardware.3. Training Infrastructure & MonitoringProblem Solved:Standard GAN training often diverges or oscillates. WGAN-GP requires precise hyperparameter tuning and monitoring.Approach:Two-Time-Scale Update Rule (TTUR): Implemented distinct learning rates for Generator ($1e^{-4}$) and Critic ($4e^{-4}$) to ensure the Critic stays ahead of the Generator .Scheduler: Applied Cosine Decay learning rate schedules to fine-tune convergence in later epochs .Checkpointing: Automated system to save/restore model states, allowing training resumption after interruptions .Visualization: Fixed-seed GIF generation to visualize the evolution of grain structures over time .Key Achievements:Robust training loop that automatically recovers from Colab timeouts.Clear visual documentation of the "coarse-to-fine" learning process via animated GIFs.4. Technical Implementation & OptimizationDeep Learning Stack:TensorFlow 2.x / Keras: Core framework for graph construction.Mixed Precision (AMP): Enabled mixed_float16 policy to reduce VRAM usage by ~50% and accelerate training throughput on NVIDIA GPUs.Custom Training Step: Overrode standard train_step to implement the specific WGAN-GP logic (5 Critic updates per 1 Generator update) .Advanced Features:Gradient Penalty: Manually computed gradients of the Critic with respect to interpolated images to stabilize training.Loss Scaling: Integrated LossScaleOptimizer to prevent underflow during mixed-precision training .5. Key Results & PerformanceVisual Quality: Successfully synthesized 64$\times$64 microstructures indistinguishable from ground truth Voronoi tessellations by Epoch 100.Training Stability: Critic loss converged stably to $\approx -25$, providing a reliable signal for training progress.Efficiency: High-performance pipeline capable of training on T4 GPUs within reasonable timeframes due to AMP optimizations.Impact:Enables rapid generation of synthetic material datasets for training downstream ML models (e.g., property prediction).Demonstrates mastery of advanced generative AI concepts beyond standard image synthesis.6. Usage & SetupPrerequisites:Google Colab (T4 GPU recommended) or Local CUDA Environment.Dataset: Voronoi H5 files (provided in repository).Execution:Setup:Python# Install dependencies
!pip install imageio tensorflow-docs
Run Training:Execute the notebook cells sequentially. The script handles data unzipping, model build, and training loops automatically.Output:Checkpoints saved to ./checkpoints.Progress frames saved to ./gif_frames.Final animation saved as voronoi_training_animation.gif.
